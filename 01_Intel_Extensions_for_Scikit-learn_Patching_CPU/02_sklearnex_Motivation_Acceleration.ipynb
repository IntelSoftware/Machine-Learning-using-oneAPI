{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4494c78-7e91-407b-833b-87e5c0abd22a",
   "metadata": {},
   "source": [
    "# Module 01_02: Motivation/ Acceleration using Extensions for Scikit-learn*\n",
    "\n",
    "![Assets/scklearnSpeedupTraining.png](Assets/scklearnSpeedupTraining.png)\n",
    "\n",
    "<a id='Back_to_Sections'></a>\n",
    "\n",
    "## Sections\n",
    "- [Introducing Intel速 AI Tools](#Intel-AI-Tools)\n",
    "- [Sklearnex Optimized Functions](#Sklearnex-Optimized-Functions)\n",
    "- [Patching Strategies with Intel(r) Extensions for scikit-learn](#Patching-Strategies-with-Intel(r)-Extensions-for-scikit-learn)\n",
    "- [Plot KNN speed up using patch](#Plot-KNN-speed-up-using-patch)\n",
    "\n",
    "\n",
    "# Dependences:\n",
    "\n",
    "Prior to labs, use pip to installed the dependencies in the requirements.txt file\n",
    "- pip install -r requirements.txt\n",
    "\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "By the end of this lession you will be able to:\n",
    "\n",
    "- Describe the basics of Intel AI Tools\n",
    "- Describe where to download and how to install Intel(R) Extension for scikit-learn*\n",
    "- Describe the advantages of Intel(R) Extensions for scikit-learn*, invoked via the sklearnex library\n",
    "- Apply the patch and unpatch functions with varying granularities including python scripts and also within Jupyter cells: from whole file applications to more surgical patches applied to a single algorithm.\n",
    "- Enumerate sklearn algorithms which have been optimized\n",
    "\n",
    "\n",
    "## Intel AI Tools\n",
    "\n",
    "![Assets/AIAnalyticsToolkitOverview.jpg](Assets/AIAnalyticsToolkitOverview.jpg)\n",
    "\n",
    "- [Back to Sections](#Back_to_Sections)\n",
    "\n",
    "The Intel速 AI Tools gives data scientists, AI developers, and researchers who are familiar Python* tools and frameworks the ability to accelerate end-to-end data science and analytics pipelines on Intel速 architectures. The components are built using oneAPI libraries for low-level compute optimizations. This toolkit maximizes performance from preprocessing through machine learning, and provides interoperability for efficient model development.\n",
    "\n",
    "The value: Using these tools, you can:\n",
    "\n",
    " - Deliver high-performance deep learning (DL) training on Intel速 XPUs and integrate fast inference into your AI development workflow with Intel-optimized DL frameworks: TensorFlow* and PyTorch*, pretrained models, and low-precision tools. \n",
    " - Achieve drop-in acceleration for data preprocessing and machine learning workflows with compute-intensive Python* packages: Modin*, scikit-learn*, and XGBoost* optimized for Intel.\n",
    " - Gain direct access to Intel analytics and AI optimizations to ensure that your software works together seamlessly.\n",
    " - This module will focus exclusively on the value aforded via the Intel(r) Extensions for scikit-learn*\n",
    " \n",
    "* download and install the Intel Extension for Scikit-learn*\n",
    "  * [Information about acquiring Intel Extension for Scikit-learn*](https://pypi.org/project/scikit-learn-intelex/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04719f4d-24e3-43fb-a042-9baf824b9466",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Describe the value \n",
    "\n",
    "### of Intel(r) Extensions for scikit-learn*\n",
    "\n",
    "See this description at anaconda.com\n",
    "\n",
    "![Scikit-learn Speed-up with Intel and Anaconda](Assets/scklearnSpeedupTraining.png)\n",
    "\n",
    "# Summarizing:\n",
    "\n",
    "Anaconda and Intel are collaborating to build key open-source data science packages optimized for Intel hardware to make machine learning fast and scalable for practitioners everywhere. \n",
    "\n",
    "How would you characterize the average gain using Intel(r) Extensions for scikit-learn* based on what you observe in this chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db43cf0-b4f1-4cfa-b157-636012eacbb9",
   "metadata": {},
   "source": [
    "# Sklearnex Optimized Functions\n",
    "\n",
    "Intel has created an better performing and functionally equivalent library contained patched versions of 32 popular scikit-learn* algorithms. To access these optimized alogirthms which are drop in replaceable with their stock counterparts, you need to:\n",
    "\n",
    "* download and install the onaAPI AI toolkit\n",
    "  *  [Information about acquiring oneAPI AI Toolkit](https://intel.github.io/scikit-learn-intelex/installation.html)\n",
    "* import the library \n",
    "```python\n",
    "     from sklearnex import patch_sklearn\n",
    "     patch_sklearn()\n",
    "```\n",
    "* THEN import the deisred sklearn library\n",
    "\n",
    "These are the [currently optimized functions optimized with Intel(R) Extensions for scikit learn](https://intel.github.io/scikit-learn-intelex/algorithms.html). Different algorithms have been optimized for Intel CPU and Intel GPU. This allows a develoeprs to check which functions are currently optimized. Some functions are simply aliases to others in that list, to see the 23 optimized \"unique\" functions - open the data/sklearnex_gallery.csv or run the follow cell.\n",
    "\n",
    "- [Back to Sections](#Back_to_Sections)\n",
    "\n",
    "Below you can interact with a Pandas Dataframe containing more information oabout the algorithms optimized using Intel Extensions for scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8115121-0c4c-4c37-a4e1-6dd796349e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sklearnex_gallery = pd.read_csv('data/sklearnex_gallery.csv')\n",
    "sklearnex_gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62ab50-2daf-4e47-aa97-d195857484d8",
   "metadata": {},
   "source": [
    "Below you can filter the dataframe to see details about sklearnex library (aka Intel Extensions to scikit-learn*)\n",
    "\n",
    "- familiarize yourself with a handful of algorithms, explore details aboput 'What is it used for', 'Advantages', 'Disadvantages'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bed106-266a-4496-a5ae-54d1b4553ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gallery_details(acroynm, column):\n",
    "    print(sklearnex_gallery[sklearnex_gallery['Acronym'] == acroynm][column].tolist()[0])\n",
    "\n",
    "details = ['What is it used for', 'Advantages', 'Disadvantages']\n",
    "print_gallery_details('dbscan', 'Advantages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab72ef-0767-44cf-b95d-a211ef074d80",
   "metadata": {},
   "source": [
    "Import the get_patch_names, get_patch_map from sklearnex and familiarize yourself with the functions available and more detials about where they reise on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd31f0-d1e0-43b7-8b13-4450aaa17565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list of optimzed functions\n",
    "# this method of finding name info can be confusing as there are aliases to multiple fuctions\n",
    "from sklearnex import get_patch_names, get_patch_map\n",
    "sorted(get_patch_names())   #get_patch_map() for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1a29b-1f03-4874-934e-8f5424ddaf1f",
   "metadata": {},
   "source": [
    "# Patching Strategies with Intel(r) Extensions for scikit-learn\n",
    "\n",
    "There are blunt/coarse methods t patch entire python scripts from the command line as well as finer granularity methods ising the patch_sklearn() down to almost surgical granularity methods of specifying which functions you wish to patch or unpatch\n",
    "\n",
    "\n",
    "- [Back to Sections](#Back_to_Sections)\n",
    "\n",
    "### patch an entire python script\n",
    "\n",
    "Without editing the code of a scikit-learn application by using the following command line flag:\n",
    "\n",
    "- python -m sklearnex my_application.py\n",
    "\n",
    "\n",
    "### to patch a Jupyter notebook cell\n",
    "\n",
    "The order of steps is important here:\n",
    "* import the sklearnex library\n",
    "* patch_sklearn()\n",
    "* import any of the sklearn libraries you wish to use - **AFTER the call to patch_sklearn()** for example:\n",
    "  * from sklearnex.neighbors import NearestNeighbors, PCA, Kmeans\n",
    "\n",
    "\n",
    "### to UNPATCH sklearn to restore the stock behavior do the following:\n",
    "\n",
    "The process is the same as for patching:\n",
    "- unpatch_sklearn()\n",
    "- next, **Re-import scikit-learn algorithms after the unpatch**\n",
    "- from sklearn.cluster import PCA\n",
    "\n",
    "### You can also specify which algorithms to patch explicitly\n",
    "\n",
    "Patching only one algorithm:\n",
    "\n",
    "- from sklearnex import patch_sklearn\n",
    "- patch_sklearn(\"SVC\")\n",
    "\n",
    "### To patch several algorithms explicitly\n",
    "\n",
    "- from sklearnex import patch_sklearn\n",
    "\n",
    "- patch_sklearn([\"SVC\", \"DBSCAN\"])\n",
    "\n",
    "### to UNPATCH algorithms explicitly, try one of these methods:\n",
    "\n",
    "- unpatch_sklearn(\"KMeans\") \n",
    "\n",
    "- unpatch_sklearn([\"KMeans\",\"SVC\"]) \n",
    "\n",
    "## FAQ: What should I do if a patched function is SLOWER than the stock version?\n",
    "\n",
    "#### Unpatch it!\n",
    "\n",
    "The ability to patch and unpatch at different granularities ensures that your code shoudl perform no slower than stock, but with potentially large upside performance opportunities. unpatch a single function that is slower or a list - you have complete control!\n",
    "\n",
    "\n",
    "- [Back to Sections](#Back_to_Sections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c397227-d25e-4084-84c6-534d9877431c",
   "metadata": {},
   "source": [
    "# The Comparison function\n",
    "\n",
    "Below we create a test harness to test specific comparisons of algorith/parameter choice with specific synthetic data generation choices\n",
    "\n",
    "For measurements, we made a function, \"comparison\",  in which data for each allgorithm is generated, an estiamtor for the algorithms is creaed, the the aggregated training and prediction times are computed.\n",
    "\n",
    "This is not an examaple of how to code but rather a test harness to showcase different functions and their speedups using the patch strategy.\n",
    "\n",
    "We define a comparison function that will pair one algorithms with a synthesized dataset, to give you a sense of how fast these allgorithms can be when applied to some datasets - Not all datasets will have the same speedups but these are likley typical for the size, shape, and complexity of the generated data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9316ea8-7d30-44ed-9b88-676a6c4413ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # progress bar\n",
    "import time #so we can time the action\n",
    "def comparison(cases):\n",
    "    elapsed_fit = {}  # dictionary to track the time elapsed for the fit method\n",
    "    elapsed_predict = {}  # dictionary to track the time elapsed for the predict/transform method \n",
    "    # the parmeters for this algorithms and for generating the data will be in the next cell\n",
    "    for name, case in tqdm(cases.items()):\n",
    "        algorithm = case['algorithm']\n",
    "        estimator = algorithm['estimator'](**algorithm['params'])\n",
    "        data = case['data']\n",
    "        x, y = data['generator'](**data['params'])\n",
    "        \n",
    "        # Timing of fit and predict    \n",
    "        start = time.time()\n",
    "        estimator.fit(x, y)\n",
    "        fit_time = time.time() - start\n",
    "        start = time.time()\n",
    "        if hasattr(estimator, 'predict'):\n",
    "            estimator.predict(x)\n",
    "        if hasattr(estimator, 'transform'):\n",
    "            estimator.transform(x)            \n",
    "        predict_time = time.time() - start\n",
    "        \n",
    "        elapsed_fit[name] = fit_time\n",
    "        elapsed_predict[name] = predict_time\n",
    "    return elapsed_fit, elapsed_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee99a28-ad27-4063-aef8-3185e62d98f2",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "\n",
    "We create a function to return a dictionary with parameters for both the alogirthm to use as well as how the data will be generated.\n",
    "\n",
    "Here we pair a given algorithm/parameter choice to a given synthetic randomly created data choice\n",
    "\n",
    "This function **defines** the agorithm/dataset pair\n",
    "\n",
    "You can retrun later to experiment with other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ca076-43df-4358-9283-d92ddc703048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cases():\n",
    "    return {\n",
    "    'SVC with Linear Kernel': {\n",
    "        \"algorithm\": {\n",
    "            'estimator': sklearn.svm.SVC,\n",
    "            'params': {\n",
    "                'C': 1.0,\n",
    "                'kernel': 'linear',\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            'generator': sklearn.datasets.make_classification,\n",
    "            'params':\n",
    "            {\n",
    "                'n_samples': 20000,\n",
    "                'n_features': 30,\n",
    "                'n_classes': 3,\n",
    "                'n_informative': 3,\n",
    "                'random_state': 43,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'SVC with RBF Kernel': {\n",
    "        \"algorithm\": {\n",
    "            'estimator': sklearn.svm.SVC,\n",
    "            'params': {\n",
    "                'C': 1.0,\n",
    "                'kernel': 'rbf',\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            'generator': sklearn.datasets.make_classification,\n",
    "            'params':\n",
    "            {\n",
    "                'n_samples': 25000,\n",
    "                'n_features': 30,\n",
    "                'n_classes': 5,\n",
    "                'n_informative': 4,\n",
    "                'random_state': 43,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        \"algorithm\": {\n",
    "            'estimator': sklearn.linear_model.LogisticRegression,\n",
    "            'params': {\n",
    "                'n_jobs': -1,\n",
    "                'random_state': 43,\n",
    "                'max_iter': 300\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            'generator': sklearn.datasets.make_classification,\n",
    "            'params':\n",
    "            {\n",
    "                'n_samples': 1000000,\n",
    "                'n_features': 40,\n",
    "                'n_classes': 10,\n",
    "                'n_informative': 5,\n",
    "                'random_state': 43,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'KNN Classifier': {\n",
    "        \"algorithm\": {\n",
    "            'estimator': sklearn.neighbors.KNeighborsClassifier,\n",
    "            'params': {\n",
    "                'n_jobs': -1,\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            'generator': sklearn.datasets.make_classification,\n",
    "            'params':\n",
    "            {\n",
    "                'n_samples': 35000,\n",
    "                'n_features': 30,\n",
    "                'n_classes': 3,\n",
    "                'n_informative': 3,\n",
    "                'random_state': 43,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'KNN Regression': {\n",
    "        \"algorithm\": {\n",
    "            'estimator': sklearn.neighbors.KNeighborsRegressor,\n",
    "            'params': {\n",
    "                'n_neighbors': 10,\n",
    "                'n_jobs': -1,\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            'generator': sklearn.datasets.make_regression,\n",
    "            'params':\n",
    "            {\n",
    "                'n_samples': 35000,\n",
    "                'n_features': 30,\n",
    "                'random_state': 43,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'Linear Regression': {\n",
    "        \"algorithm\": {\n",
    "            'estimator': sklearn.linear_model.LinearRegression,\n",
    "            'params': {\n",
    "                'n_jobs': -1,\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            'generator': sklearn.datasets.make_regression,\n",
    "            'params':\n",
    "            {\n",
    "                'n_samples': 3000000,\n",
    "                'n_features': 100,\n",
    "                'n_targets': 60,\n",
    "                'random_state': 43,\n",
    "            }\n",
    "        }\n",
    "    },     \n",
    "    'Ridge Regression': {\n",
    "        \"algorithm\": {\n",
    "            'estimator': sklearn.linear_model.Ridge,\n",
    "            'params': {\n",
    "                'alpha':1.0\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            'generator': sklearn.datasets.make_classification,\n",
    "            'params':\n",
    "            {\n",
    "                'n_samples': 30000000,\n",
    "                'n_features': 30,\n",
    "                'n_classes': 3,\n",
    "                'n_informative': 3,\n",
    "                'random_state': 43,\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    'PCA': {\n",
    "            \"algorithm\": {\n",
    "            'estimator': sklearn.decomposition.PCA,\n",
    "            'params': {\n",
    "                'n_components': 25,\n",
    "                'svd_solver': 'full',\n",
    "                'random_state': 43,\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            'generator': sklearn.datasets.make_blobs,\n",
    "            'params':\n",
    "            {\n",
    "#                 'n_samples': 3000000,\n",
    "#                 'n_features': 100,\n",
    "                'n_samples': 3000000,\n",
    "                'n_features': 30,                \n",
    "                'centers': 20,\n",
    "                'random_state': 43,\n",
    "            }\n",
    "        }\n",
    "    },   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5270a-c03f-4d0d-b54e-09e9c5f60361",
   "metadata": {},
   "source": [
    "# Feel the burn \n",
    "###  **UN**patch_sklearn to get a **feel** for how **long** this can take with **stock sklearn**\n",
    "\n",
    "Use the code snippet below to **unpatch** sklearn\n",
    "```python\n",
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "unpatch_sklearn()  \n",
    "```\n",
    "\n",
    "Intel(R) Extension for Scikit-learn* disabled (https://github.com/intel/scikit-learn-intelex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b769b-bdd7-4d28-9b1d-d7ffc71744d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## insert unpath lines below  ##################\n",
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "unpatch_sklearn() \n",
    "############################################################\n",
    "\n",
    "import sklearn.svm, sklearn.datasets, sklearn.neighbors, sklearn.linear_model, sklearn.decomposition\n",
    "cases = get_cases()  #case the algorithm/dataset pairs\n",
    "sklearn_fit, sklearn_predict = comparison(cases)  # call the comparison function to captures the elapsed time dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967bf3d-7034-4586-9c01-ee0381461ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Result of scikit-learn stock: ', sklearn_fit)\n",
    "print('Result of scikit-learn stock: ', sklearn_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf3a360-d434-4afb-81ff-c91ed09049f2",
   "metadata": {},
   "source": [
    "# patch_sklearn\n",
    "\n",
    "Now feel the acceleration when pathcing is applied\n",
    "\n",
    "NOTICE: **No code changes**: We have not changed any code here inside the comparison. We simply patch it prior to importing our desired sklearn components!\n",
    "\n",
    "[More detail for Intel(r) Extension for Scikit-learn* enabled](https://github.com/intel/scikit-learn-intelex)\n",
    "\n",
    "**Exercise:**\n",
    "- Insert the two lines of code required to **patch** scikit-learn* as inidcated in the cell below and rerun this cell and all cells below that\n",
    "\n",
    "```python\n",
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faab517-cc45-4d57-a007-74092c9860bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code ( a single line) to patch sklearn here:\n",
    "\n",
    "#### insert code here ###########\n",
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()\n",
    "#################################\n",
    "\n",
    "import sklearn.svm, sklearn.datasets, sklearn.neighbors, sklearn.linear_model, sklearn.decomposition\n",
    "cases = get_cases()\n",
    "sklearnex_fit, sklearnex_predict = comparison(cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fabcd-535e-4e85-adae-19cbe5f55dd7",
   "metadata": {},
   "source": [
    "# Analysis of result\n",
    "\n",
    "Next we plot the timing results\n",
    "\n",
    "## Plot results for the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffe702-b067-4241-a631-67b434ccedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "result = { name_sk: [time_ex, time_sk] for (name_sk, time_sk), \n",
    "          (name_ex, time_ex) in zip(sklearn_fit.items(), sklearnex_fit.items())}\n",
    "result['Library'] = ['sklearnex', 'sklearn']\n",
    "df_fit = pd.DataFrame(data=result).melt('Library', var_name='Algorithms', value_name='Time')\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "fig.patch.set_alpha(1)\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "barplot = sns.barplot(x='Algorithms', y='Time', hue='Library',\n",
    "                      data=df_fit, errwidth = 2,\n",
    "                      capsize = 0.05, saturation = 8,)\n",
    "\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.1f'),\n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height() + 2),\n",
    "                     ha = 'center', va = 'center')\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.ylabel(\"Time (sec)\", size=14)\n",
    "plt.xlabel(\"algorithms\", size=14)\n",
    "plt.title(\"Performance result of fit (LOWER is BETTER)\")\n",
    "plt.xticks(rotation = -60) # Rotates X-Axis Ticks by 60-degrees\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4ab0f-825f-4343-bab4-ba47abed055c",
   "metadata": {},
   "source": [
    "## Plot results for the transform/predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97d84f-cdfb-4be6-96b5-468f490d81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = { name_sk: [time_ex, time_sk] for (name_sk, time_sk), \n",
    "          (name_ex, time_ex) in zip(sklearn_predict.items(), sklearnex_predict.items())}\n",
    "result['Library'] = ['sklearnex', 'sklearn']\n",
    "df_predict = pd.DataFrame(data=result).melt('Library', var_name='Algorithms', value_name='Time')\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "fig.patch.set_alpha(1)\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "barplot = sns.barplot(x='Algorithms', y='Time', hue='Library',\n",
    "                      data=df_predict, errwidth = 2,\n",
    "                      capsize = 0.05, saturation = 8,)\n",
    "\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.1f'),\n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height() + 2),\n",
    "                     ha = 'center', va = 'center')\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.ylabel(\"Time (sec)\", size=14)\n",
    "plt.xlabel(\"algorithms\", size=14)\n",
    "plt.title(\"Performance result predict/transform  (LOWER is BETTER)\")\n",
    "plt.xticks(rotation = -60) # Rotates X-Axis Ticks by 60-degrees\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50de25-bf90-4bcd-be7b-6c6df45133db",
   "metadata": {},
   "source": [
    "## Plot results for the aggreagte of fit + predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cc410-60bb-44af-969d-400555427bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = { name_sk: [time_ex, time_sk] for (name_sk, time_sk), \n",
    "          (name_ex, time_ex) in zip(sklearn_fit.items(), sklearnex_fit.items())}\n",
    "result['Library'] = ['sklearnex', 'sklearn']\n",
    "df_fit = pd.DataFrame(data=result).melt('Library', var_name='Algorithms', value_name='Time')\n",
    "\n",
    "result = { name_sk: [time_ex, time_sk] for (name_sk, time_sk), \n",
    "          (name_ex, time_ex) in zip(sklearn_predict.items(), sklearnex_predict.items())}\n",
    "result['Library'] = ['sklearnex', 'sklearn']\n",
    "df_predict = pd.DataFrame(data=result).melt('Library', var_name='Algorithms', value_name='Time')\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "fig.patch.set_alpha(1)\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "barplot = sns.barplot(x='Algorithms', y='Time', hue='Library',\n",
    "                      data= df_fit + df_predict, errwidth = 2,\n",
    "                      capsize = 0.05, saturation = 8,)\n",
    "\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.1f'),\n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height() + 2),\n",
    "                     ha = 'center', va = 'center')\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.ylabel(\"Time (sec)\", size=14)\n",
    "plt.xlabel(\"algorithms\", size=14)\n",
    "plt.title(\"Performance result predict/transform\")\n",
    "plt.xticks(rotation = -60) # Rotates X-Axis Ticks by 60-degrees\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc58cf0-98d9-416e-9728-e1280c006fc3",
   "metadata": {},
   "source": [
    "All algorithms were greatly accelerated, instead of minutes, they worked in just a few seconds! Acceleration reaches up to 100 times - imagine how simple and fast it is now to compute your kernels with scikit-learn applications using Intel(R) Extension!\n",
    "\n",
    "Go back and experiment with a handful of dataset parameters (one dataset at a time and dont make the datasizes too radically different right away - you could run out of memory or the algorithm can take forever. As you gain familiarity with how the algorithm/dataset pair behaves with changes in dataset parameters you can then gage how large to make the next change - based on your patience to wait for the stock result\n",
    "\n",
    "- n_samples\n",
    "- nfeatures\n",
    "- nclasses\n",
    "- n_informative\n",
    "\n",
    "Another suggestion: Add Logistic regression to the list using similar parameters to SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e5657-f37f-41d4-b44c-6fc0e0d18b5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "With Intel(R) Extension for Scikit-learn* patching you can:\n",
    "\n",
    "Use your scikit-learn code for training and inference without modification.\n",
    "Train and predict scikit-learn models up to 100 times faster.\n",
    "Get the same quality of predictions as other tested frameworks.\n",
    "Intel(R) Extension for Scikit-learn* adds more accelerated scikit-learn algorithms to each release, learn what algorithms are optimized [here](https://intel.github.io/scikit-learn-intelex/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743ab76-3cb4-4dd1-9e42-1ce40360e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c9b800-67db-4dbe-9bea-756a4866906d",
   "metadata": {},
   "source": [
    "# Notices & Disclaimers \n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation.\n",
    "No product or component can be absolutely secure.\n",
    "\n",
    "Your costs and results may vary.\n",
    "\n",
    "息 Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. \n",
    "*Other names and brands may be claimed as the property of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7f42f-e745-4d6e-92cb-ec7de44ce151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0af0cf-97f0-4251-928d-270e5c82058c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
