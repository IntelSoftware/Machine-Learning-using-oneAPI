{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599e019e",
   "metadata": {},
   "source": [
    "# Module 03 Practicum:  ImageClustering using PCA, Kmeans, DBSCAN\n",
    "\n",
    "In this module you will be generating this graph, and describing that it means in terms os sanity checking your clusters, and what it means in terms of feature dimensions\n",
    "\n",
    "![Assets/dbscan_graph.png](Assets/dbscan_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719ef49",
   "metadata": {},
   "source": [
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Explore and interpret the image dataset\n",
    "\n",
    "* Apply IntelÂ® Extension for Scikit-learn* patches to Principal Components Analysis (PCA), Kmeans,and DBSCAN algorithms\n",
    "* **Synthesize** your understanding- searching for ways to patch or unpatch any applicable cells to maximize the performance of each cell\n",
    "\n",
    "# Practicum:\n",
    "\n",
    "- Analyze each cell looking for places to patch or unpatch as needed to maximize the performace of each cell.\n",
    "- Apply a patching strategy that applies the patch to every algorithm optimized by patching.\n",
    "\n",
    "\n",
    "\n",
    "<a id='Back_to_Sections'></a>\n",
    "\n",
    "## Sections\n",
    "\n",
    "- _Code:_ [Read Images](#Define-image-manipulation-and-Reading-functions)\n",
    "- _Code:_ [Submit batch_clustering_Streamlined.py as a batch job](#Submit-batch_clustering_Streamlined.py-as-a-batch-job)\n",
    "- _Code:_ [Plot Kmeans using GPU results](#Plot-Kmeans)\n",
    "- _Code:_ [Plot DBSCAN using GPU results](#Plot-DBSCAN)\n",
    "\n",
    "# Dependencies required:\n",
    "\n",
    "- pip install pillow\n",
    "- pip install seaborn\n",
    "\n",
    "# Data Description\n",
    "\n",
    "This data represent wildlife images taken by the author of thisnotebook. It is an UNBALANCED set of images of Collared Lizards, Pelicans, and Seascapes. The images are shot in various white balances to make clustering a challenge.  Additionally, the data are imbalalanced so that some lizrds were photographed in bright light, and some in shade. The goal is to perform clustering to aid eventual clsasification for use in a CNN semi-supervised learning exercise\n",
    "\n",
    "The iamges are read in from a folder, and one by one that are reshaped to a single row with perhaps 60,000 columns (one image now per row). After they are assembled into a NumPy array there are scaled to zero mean, unit standard deviation to aid the Prinicpal Component step to have somewhat similar scale ranges.\n",
    "\n",
    "The image array is tranaformed via PCA with 2 to 6 principal components shich changes the shape of the data we feed to kmeans or DBSCAn to n x c where c is the number of principal components we desire (likely in range from 2 to 6)\n",
    "\n",
    "Then the data is plotted in a Seaborn pairplot to allow us to spot clusters visually, by which ever combo of principal components.\n",
    "\n",
    "We use KMeans or DBSCAN to help automaticllay select which images belong to which cluster as reppresented by the color of the dots.\n",
    "\n",
    "We also display an image grid of the clustered images to see if simialr images are clustered together to meet with our intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0d31c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T03:35:22.700065Z",
     "start_time": "2021-10-06T03:35:22.693041Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "data_path = ['data']\n",
    "\n",
    "# Notebook time start\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "current_time = start_time.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23938fb",
   "metadata": {},
   "source": [
    "# Define image manipulation and Reading functions\n",
    "\n",
    "You do not have to patch code inside the lab.Read_Transform_Images script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17e812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T23:14:00.614891Z",
     "start_time": "2021-09-30T23:14:00.603207Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()\n",
    "from lab.Read_Transform_Images import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc9b30",
   "metadata": {},
   "source": [
    "<a id='Actually-read-the-images'></a>\n",
    "# Actually read the images\n",
    "\n",
    "- [Back to Sections](#Back_to_Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935125a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T03:35:32.344246Z",
     "start_time": "2021-10-06T03:35:24.446381Z"
    }
   },
   "outputs": [],
   "source": [
    "resultsDict = {}\n",
    "#resultsDict = Read_Transform_Images(resultsDict,imagesFilenameList = imagesFilenameList)\n",
    "resultsDict = Read_Transform_Images(resultsDict,path = 'data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdfcd12-03d9-40e3-b245-6845189a3ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T03:19:44.061110Z",
     "start_time": "2021-10-06T03:19:44.053079Z"
    }
   },
   "source": [
    "# Display ImageGrid Random Sampling\n",
    "\n",
    "This should give an idea of how closely or differently the various images appear. Notice that some of the collard lizard images have much differnet white balance and this will affect the clustering. For this dataset the images are clustered based on the similarity in RGB colorspace only.\n",
    "\n",
    "You should notice several similar images of:\n",
    "- Dark white balance lizards\n",
    "- Light white balance lizards\n",
    "- Skyscape with pelicans\n",
    "- Seascapes with surf\n",
    "\n",
    "Ccan an algorithm we design find clustering values in rough agreement to the above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28840f2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T03:35:36.643550Z",
     "start_time": "2021-10-06T03:35:32.344246Z"
    }
   },
   "outputs": [],
   "source": [
    "img_arr = []\n",
    "ncols = 8\n",
    "imageGrid=(ncols,3)\n",
    "for pil in random.sample(resultsDict['list_PIL_Images'], imageGrid[0]*imageGrid[1])  :\n",
    "    img_arr.append(np.array(pil))\n",
    "#displayImageGrid(img_arr, imageGrid=imageGrid)\n",
    "displayImageGrid2(img_arr, ncols=ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e099226-f1ed-48a9-b00e-347f46fe1d1c",
   "metadata": {},
   "source": [
    "## Create a Cell in the Style of a main() Python Script\n",
    "\n",
    "This is a preperatory step for a later  when we qsub code to another compute node that has a GPU resiing in it\n",
    "\n",
    "In this case we run the code on a CPU, so we can become familiar with the results for comparison to later GPU results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12826de6-079c-49ea-986a-52c4499a7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# %load batch_clustering_Streamlined.py\n",
    "# batch_clustering_Streamlined.py\n",
    "\n",
    "#===============================================================================\n",
    "# Copyright 2014-2022 Intel Corporation\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#===============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from PIL.Image import Image as PilImage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "import random\n",
    "import operator\n",
    "import seaborn as sns\n",
    "\n",
    "from lab.Read_Transform_Images import ReshapeShortFat\n",
    "from lab.Read_Transform_Images import Read_Transform_Images\n",
    "from lab.Read_Transform_Images import displayImageGrid\n",
    "from lab.Read_Transform_Images import NumpyEncoder\n",
    "\n",
    "\n",
    "def main():\n",
    "    #from sklearn.decomposition import PCA\n",
    "\n",
    "    resultsDict = {}\n",
    "    #resultsDict = Read_Transform_Images(resultsDict)\n",
    "    resultsDict = Read_Transform_Images(resultsDict,path = 'data/')\n",
    "    \n",
    "    knee = 6\n",
    "    EPS = 220\n",
    "    n_components = 3\n",
    "    n_samples = 3\n",
    "    NP_images_STD = resultsDict['NP_images_STD'] # images as numpy array\n",
    "\n",
    "    del resultsDict\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data = NP_images_STD\n",
    "    PCA_fit_transform = pca.fit_transform(data) \n",
    "    print(PCA_fit_transform[:3,:])\n",
    "    k_means = KMeans(n_clusters = knee, init='random')\n",
    "    db = DBSCAN(eps=EPS, min_samples = n_samples).fit(PCA_fit_transform)\n",
    "    km = k_means.fit(PCA_fit_transform)     \n",
    "        \n",
    "    print('db.labels_ ', db.labels_)\n",
    "    print('km.labels_ ',km.labels_)\n",
    "    \n",
    "    PCA_table = pd.DataFrame(PCA_fit_transform) \n",
    "    PCA_table.to_csv('data/PCA_fit_transform.csv', index=False )\n",
    "    db_table = pd.DataFrame(db.labels_) \n",
    "    db_table.to_csv('data/db_labels.csv', index=False )    \n",
    "    km_table = pd.DataFrame(km.labels_) \n",
    "    km_table.to_csv('data/km_labels.csv', index=False )\n",
    "    counts_db, bins_db = np.histogram(db.labels_, bins = knee)\n",
    "    counts_db_table = pd.DataFrame(counts_db)\n",
    "    counts_db_table.to_csv('data/counts_db.csv', index=False ) \n",
    "    bins_db_table = pd.DataFrame(bins_db)\n",
    "    bins_db_table.to_csv('data/bins_db.csv', index=False ) \n",
    "    \n",
    "    counts_km, bins_km =np.histogram(k_means.labels_, bins = knee)\n",
    "    counts_km_table = pd.DataFrame(counts_km)\n",
    "    counts_km_table.to_csv('data/counts_km.csv', index=False ) \n",
    "    bins_km_table = pd.DataFrame(bins_km)\n",
    "    bins_km_table.to_csv('data/bins_km.csv', index=False ) \n",
    "    \n",
    "    print(\"All good inside main\\n\")\n",
    "    \n",
    "    return \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    #print(\"km_list: \", resultsDict['km_list'][0:2])\n",
    "    print('All looks good!\\nRun 03_Plot_GPU_Results.ipynb to graph the results!')\n",
    "\n",
    "# Notices & Disclaimers \n",
    "\n",
    "# Intel technologies may require enabled hardware, software or service activation.\n",
    "# No product or component can be absolutely secure.\n",
    "\n",
    "# Your costs and results may vary.\n",
    "\n",
    "# Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. \n",
    "# *Other names and brands may be claimed as the property of others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dab435-4c4d-478b-bc84-c6803de0ce1d",
   "metadata": {},
   "source": [
    "Unpatched\n",
    "- CPU times: user 16 s, sys: 1.08 s, total: 17.1 s\n",
    "- Wall time: 1.73 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b4cd0-1c55-4bfc-b670-c52cdcbd7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = '../03_Applied_to_Image_Clustering_CPU/data/'\n",
    "resultsDict = {}\n",
    "resultsDict = Read_Transform_Images(resultsDict, path=path)\n",
    "\n",
    "knee = 6\n",
    "EPS = 230\n",
    "n_components = 3\n",
    "n_samples = 3\n",
    "    \n",
    "pca = PCA(n_components=n_components)\n",
    "data = resultsDict['NP_images_STD']\n",
    "PCA_fit_transform = pca.fit_transform(data) \n",
    "k_means = KMeans(n_clusters = knee, init='random')\n",
    "db = DBSCAN(eps=EPS, min_samples = n_samples).fit(PCA_fit_transform)\n",
    "km = k_means.fit(PCA_fit_transform) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dad8b4-c9f4-4020-97ae-eedcde391d71",
   "metadata": {},
   "source": [
    "Unpatched\n",
    " - CPU times: user 17.7 s, sys: 996 ms, total: 18.7 s\n",
    "- Wall time: 1.69 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5bafe-42bd-45c3-82bd-3aa54b65d0c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='Plot-Kmeans'></a>\n",
    "# Plot Kmeans Clusters \n",
    "\n",
    "Plot a histogram of the using GPU results\n",
    "-  [Back to Sections](#Back_to_Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a00148-a8dd-460e-9721-39aa52655a5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T03:36:06.710515Z",
     "start_time": "2021-10-06T03:36:06.475484Z"
    }
   },
   "outputs": [],
   "source": [
    "#resultsDict = Compute_kmeans_db_histogram_labels(resultsDict, knee = 6, gpu_available = gpu_available) #knee = 5\n",
    "counts = np.asarray(pd.read_csv('data/counts_km.csv'))\n",
    "bins = np.asarray(pd.read_csv('data/bins_km.csv'))\n",
    "#counts = np.asarray(resultsDict['counts'])\n",
    "#bins = np.asarray(resultsDict['bins'])\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Histogram with Probability Plot: Context {}\".format('device_context'))\n",
    "slice = min(counts.shape[0], bins.shape[0])\n",
    "plt.bar(bins[:slice,0],counts[:slice,0])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96d3fc-7ce3-4fcf-af37-7976bf4dca3e",
   "metadata": {},
   "source": [
    "# Display Similar Images\n",
    "\n",
    "Visually compare image which have been clustered by the allgorithm.\n",
    "\n",
    "You may be surpirsed at the cluster grouping - what kmeans finds simialr or dissimilar based only on RGB colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a430c-3a61-4b43-9d33-e0d0f056bc72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T03:36:10.547299Z",
     "start_time": "2021-10-06T03:36:06.712486Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusterRank = 0\n",
    "km_labels = np.asarray(pd.read_csv('data/km_labels.csv'))\n",
    "d = {i:cts for i, cts in enumerate(np.asarray(pd.read_csv('data/counts_km.csv')))}\n",
    "sorted_d = sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "id = sorted_d[clusterRank][0] \n",
    "indexCluster = np.where(km_labels == id  )[0].tolist()\n",
    "img_arr = []\n",
    "for idx in indexCluster:\n",
    "    img_arr.append(np.array((resultsDict['list_PIL_Images'][idx])))\n",
    "img_arr = np.asarray(img_arr)\n",
    "displayImageGrid(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac085d2a-89ce-4b76-a1f2-78d3ddfd53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b7104-05eb-45a4-b363-182f37d94f13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T03:36:10.594867Z",
     "start_time": "2021-10-06T03:36:10.549295Z"
    }
   },
   "source": [
    "# Plot Seaborn Kmeans Clusters\n",
    "\n",
    "Indicates numbers of images that are close in color space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da733b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T03:36:17.302892Z",
     "start_time": "2021-10-06T03:36:10.596866Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "n_components = 3\n",
    "\n",
    "PCA_table = pd.read_csv('data/PCA_fit_transform.csv')\n",
    "columns = ['PC{:0d}'.format(c) for c in range(n_components)]\n",
    "data = pd.DataFrame(np.asarray(PCA_table)[:,:n_components], columns = columns)\n",
    "#k_means = resultsDict['model']\n",
    "data['cluster'] = km_labels\n",
    "data.head()\n",
    "\n",
    "columns.append('cluster')\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "sns.set_context('notebook');\n",
    "g = sns.pairplot(data[columns], hue=\"cluster\", palette=\"Paired\", diag_kws=dict(hue=None));\n",
    "g.fig.suptitle(\"KMEANS pairplot: Context {}\".format('device_context'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117eff7-5f57-473d-91a5-9adf541cadff",
   "metadata": {},
   "source": [
    "# Find DBSCAN EPS parameter\n",
    "\n",
    "**We need to know roughly the size of EPS to try** \n",
    "\n",
    "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n",
    "\n",
    "EPS: \"epsilon\" value in sklearn is the maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "\n",
    "At least a first value to start. We are using kNN to find distances commonly occuring in the dataset. Values of EPS below this threshold distance will be considered as lyig within a given cluster. This means we should look for long flat plateaus and read the y coordinate off the kNN plot to get a starting value for EPS.\n",
    "\n",
    "Different datasets can have wildly different sweet spots for EPS. Some datasets require EPS values of .001 other datasets may work best with values of EPS of several thousand. We use this trick to get in the right or approximate neighborhood of the EPS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7563130-17e1-4ea1-bbb7-503ddee77d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "PCA_images = data\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=2)\n",
    "#X = StandardScaler().fit_transform(PCA_images)\n",
    "neighbors_fit = neighbors.fit(PCA_images)\n",
    "distances, indices = neighbors_fit.kneighbors(PCA_images)\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "plt.xlabel('number of images')\n",
    "plt.ylabel('distances')\n",
    "plt.title('Nearest Neighbor Distances Plot')\n",
    "plt.plot(distances[:,1])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e95a2-745c-418a-8418-dc215b74328d",
   "metadata": {},
   "source": [
    "Unpatched\n",
    "\n",
    "- CPU times: user 17.2 ms, sys: 85 Âµs, total: 17.2 ms\n",
    "- Wall time: 13.8 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5bbc10-0d01-47e8-b6f4-5abaf159de1c",
   "metadata": {},
   "source": [
    "# Use DBSCAN to find clusters\n",
    "\n",
    "Use initial estiamtes from KNN above (find elbow) to given initial trial for DBSCAN EPS values \n",
    "\n",
    "In the plot above, there is a plateau in the y values somewherre near 350 indicating that a cluster distance (aka EPS) might work well somewhere near this value. We used this value in the batch_clustering_Streamlined.py file when computing DBSCAN.\n",
    "\n",
    "**EPS:** Two points are  neighbors if the distance between the two points is below a threshold.\n",
    "**n:** The minimum number of neighbors a given point should have in order to be classified as a core point. \n",
    "The point itself is included in the minimum number of samples.\n",
    "\n",
    "# Below: Find EPS That Minimizes Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a17dbb-9d03-4af9-be31-b20bdd391032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "data = resultsDict['NP_images_STD']\n",
    "PCA_fit_transform = pca.fit_transform(data) \n",
    "n_samples = 2 # we'll say that you have to have at least 2 points in a set to call it a cluster\n",
    "\n",
    "R = [i for i in range(20,460, 5)] # Range of EPS values to try\n",
    "v = np.zeros([len(R), 2]) # empty array\n",
    "idx = 0\n",
    "plotData = np.zeros([len(R),2])\n",
    "for eps in  R:\n",
    "    db = DBSCAN(eps = eps, min_samples = n_samples).fit(PCA_fit_transform)\n",
    "    counts = np.unique(db.labels_ , return_counts=True)\n",
    "    plotData[idx,0] = eps\n",
    "    plotData[idx,1] = counts[1][0]\n",
    "    idx += 1\n",
    "plt.plot(plotData[:,0], plotData[:,1])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2fa1d-9c33-404f-98ff-c670867ebbc8",
   "metadata": {},
   "source": [
    "Unpatched\n",
    "\n",
    "- CPU times: user 12.4 s, sys: 629 ms, total: 13 s\n",
    "- Wall time: 1.18 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0cd669-2581-4f5c-af1b-8292eb1fd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%%write_and_run    lab/compute_DBSCANClusterRank.py \n",
    "def compute_DBSCANClusterRank(n_samples = 3, n_components = 3, EPS = 230 ):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data = resultsDict['NP_images_STD']\n",
    "    PCA_fit_transform = pca.fit_transform(data) \n",
    "    db = DBSCAN(eps=EPS, min_samples = n_samples).fit(PCA_fit_transform)\n",
    "\n",
    "    counts = np.unique(db.labels_ , return_counts=True)\n",
    "    d = {k:v for k, v in sorted(zip(counts[0], counts[1]), reverse=True, key=operator.itemgetter(1))}\n",
    "    sorted_d = {k:v for k, v in sorted(zip(counts[0], counts[1]), reverse=True, key=operator.itemgetter(1))}\n",
    "    return db, counts, sorted_d, PCA_fit_transform\n",
    "data\n",
    "n_components = 3\n",
    "columns = ['PC{}'.format(i) for i in range(n_components)]\n",
    "EPS = 230\n",
    "db, counts, sorted_d, PCA_fit_transform = compute_DBSCANClusterRank(n_samples = 3, n_components = n_components, EPS = EPS)\n",
    "PCAdf = pd.DataFrame(PCA_fit_transform, columns = columns)\n",
    "PCAdf['cluster'] = db.labels_\n",
    "print(sorted_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b8bc7-b940-4908-acf6-60ddf013187d",
   "metadata": {},
   "source": [
    "Unpatched\n",
    "\n",
    "- CPU times: user 11.5 s, sys: 680 ms, total: 12.2 s\n",
    "- Wall time: 1.09 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69a78c-c77e-43a6-967b-bd493195b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set_context('notebook');\n",
    "g = sns.pairplot(PCAdf, hue=\"cluster\", palette=\"Paired\", diag_kws=dict(hue=None));\n",
    "g.fig.suptitle(\"DBSCAN pairplot: Context {}\".format('device_context'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a43d8-b6fe-47ca-8f35-b28edd664dea",
   "metadata": {},
   "source": [
    "# Print Filenames of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283baef-d9c1-4e5c-b7a9-2198c741d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Outlier/problematic images are: \\n', \n",
    "      [resultsDict['imagesFilenameList'][f].split('/')[-1] for f in list(PCAdf[PCAdf['cluster'] == -1].index)]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95f1b7-dd09-4513-9771-9254f84a9f7e",
   "metadata": {},
   "source": [
    "<a id='Plot-DBSCAN'></a>\n",
    "# DBSCAN Cluster Plots\n",
    "\n",
    "Plot a histogram of the using GPU results\n",
    "- [Back to Sections](#Back_to_Sections)\n",
    "\n",
    " \n",
    "\n",
    "To indicate numbers of images in each cluster. color each point by its membership in a cluster\n",
    "\n",
    "**Why read the counts and bins for DB from the csv from earlier?**\n",
    "\n",
    "We will read the values from those saved earlier into a couple of csv files. THis is so later, when we modify for use with GPU on another node in the cluster, we can plot the results from that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92269be2-2bff-47f9-b330-f5e88e2b2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resultsDict = Compute_kmeans_db_histogram_labels(resultsDict, knee = 6, gpu_available = gpu_available) #knee = 5\n",
    "counts = np.asarray(pd.read_csv('data/counts_db.csv'))\n",
    "bins = np.asarray(pd.read_csv('data/bins_db.csv'))\n",
    "#counts = np.asarray(resultsDict['counts'])\n",
    "#bins = np.asarray(resultsDict['bins'])\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Histogram with Probability Plot: Context {}\".format('device_context'))\n",
    "slice = min(counts.shape[0], bins.shape[0])\n",
    "plt.bar(bins[:slice,0],counts[:slice,0])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effabc45-ac33-4f9f-b354-33b9df4d0ab4",
   "metadata": {},
   "source": [
    "### We sort the dictionary containing the idx and counts of each cluster\n",
    "\n",
    "We call the most frequently encoutered cluster, ClusterRank 0.  The next most frequently encoutered cluster,, is ClusterRank1, etc\n",
    "\n",
    "Once we know the most common clusters, we can arrange the photos in a grid so we can see iamge clusters ot get a visual sense of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4078d85-1dfa-4426-b2f6-b3acd74573cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterRank = 2\n",
    "db_labels = np.asarray(pd.read_csv('data/db_labels.csv'))\n",
    "d = {i:cts for i, cts in enumerate(np.asarray(pd.read_csv('data/counts_db.csv')))}\n",
    "sorted_d = sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "id = sorted_d[clusterRank][0] - 1\n",
    "indexCluster = np.where(db_labels == id  )[0].tolist()\n",
    "img_arr = []\n",
    "for idx in indexCluster:\n",
    "    img_arr.append(np.array((resultsDict['list_PIL_Images'][idx])))\n",
    "img_arr = np.asarray(img_arr)\n",
    "displayImageGrid(img_arr)\n",
    "print('id: ',id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43c36b-7574-438f-9114-31694fc74dbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notes:\n",
    "\n",
    "Did you notice any differences in timing between patched versus unpatched?\n",
    "\n",
    "If you exerpereinced marginal or no performance improvements, What do you think explains your observation?\n",
    "\n",
    "\n",
    "# Final Thoughts and next steps...\n",
    "\n",
    "You may have noticed how difficult it is to get decent clustering on even 30 or 40 images using only RGB or HSV as the feature set\n",
    ".\n",
    "If all the images are well separated in either RGB, or HSV color space, then these features are useful for clustering.\n",
    "\n",
    "However, a suggested next stepâor next trainingâwould be to encode the data differently. Perhaps using image classification with VGG16, but removing the last layer as a preprocess prior to k-means or DBSCAN.\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2b8dd-1cf5-4645-ad4c-c8855a35eacf",
   "metadata": {},
   "source": [
    "# Notices & Disclaimers \n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation.\n",
    "No product or component can be absolutely secure.\n",
    "\n",
    "Your costs and results may vary.\n",
    "\n",
    "Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. \n",
    "*Other names and brands may be claimed as the property of others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd7ccf-f6fd-4d33-aad9-ce6477e9c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (IntelÂ® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
